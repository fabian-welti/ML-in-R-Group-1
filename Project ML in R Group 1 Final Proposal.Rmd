---
title: "Project ML in R Group 1 V6.0"
author: "Alessandro Imbrogno, Edward Raynar, Thomas Qvist Bucher-Johannessen, Fabian Welti"
date: "24.11.2025"
output: word_document
---

### Motivation and Relevance

Modern agriculture must increase productivity while operating under limited resources and increasingly variable environmental conditions. Machine learning offers a practical way to understand how soil nutrients, climate, and environmental conditions shape crop performance. Accurate yield prediction can support better fertilizer planning, irrigation decisions, and risk management, contributing to more efficient and sustainable farming.

We work with the Crop Yield Prediction dataset from Kaggle, which contains multiple crop types and key agronomic variables including nitrogen, phosphorus, potassium, temperature, humidity, pH, rainfall, and yield. Before modeling, we conduct an exploratory analysis to review distributions, correlations, and data quality in order to build a reliable foundation for the modeling steps that follow.

Our modeling workflow applies several supervised learning methods covered in the course, including a featureless baseline, linear regression, a CART regression tree, and a random forest. Using the mlr3 framework, we compare these models under identical resampling conditions through a structured benchmarking setup. For the standard k fold cross validation setting, we also apply nested resampling. A central part of the analysis is the distinction between two predictive settings. First, we evaluate how well the models predict yield for crop types already seen during training using standard k fold cross validation. Second, we apply blocked cross validation by crop to test whether the models can generalize to entirely unseen crop types, which reflects real world cases where new varieties or hybrids appear.

These two settings lead to our research question: **how accurately can crop yield be predicted from soil and environmental characteristics, and does this predictive accuracy hold when applied to entirely new crop types?**

After benchmarking, we train the best performing model on the full dataset and compute permutation based variable importance to identify which environmental factors most strongly influence predictions. This links the results back to practical agronomy and helps interpret not only how well the models perform but also which features drive that performance.


## Preparatory Steps
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Housekeeping
rm(list=ls())

## Install & Load Packages
packages <- c("ggplot2", "ggcorrplot", "dplyr", "moments", "mlr3", "mlr3learners", "mlr3tuning", 
              "rpart.plot", "iml", "rstudioapi", "parallel", "future", "tidyr", "paradox")

# Install missing packages
installed <- packages %in% rownames(installed.packages())
if (any(!installed)) {
  install.packages(packages[!installed], dependencies = TRUE)
}

# Load all packages
lapply(packages, library, character.only = TRUE)
packages <- c("ggplot2", "ggcorrplot", "dplyr", "moments", "mlr3", "mlr3learners", "mlr3tuning",  
              "rpart.plot", "iml", "rstudioapi", "parallel", "future", "tidyr", "paradox")
lapply(packages, library, character.only = TRUE)

# Set working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))  
# Set seed
set.seed(123)
```

## Data Loading & Exploratory Analysis
#Data Loading & EDA
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Load dataset
df <- read.csv("Crop_Yield_Prediction.csv")
df$Crop <- as.factor(df$Crop)  # Convert Crop to factor

# Initial data overview
head(df)
tail(df)
str(df)
summary(df)

# Unique crops and counts
unique(df$Crop)
count(df, Crop)

# Missing values
na_count <- sapply(df, function(j) sum(is.na(j)))
print(na_count)  # No NA values -> imputations won't be necessary 

# Average values per crop
crop_summary <- aggregate(. ~ Crop, data = df, FUN = mean, na.rm = TRUE)
print(crop_summary)
```

#Crop distribution visualization
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
crop <- ggplot(df, aes(x = Crop)) +
  geom_bar(fill = "darkgreen") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Number of Observations per Crop Type")

crop
```
#Mean values of all variables by crop
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
crop_summary <- df %>%
  group_by(Crop) %>%
  summarise(across(
    c(Nitrogen, Phosphorus, Potassium, Temperature,
      Humidity, pH_Value, Rainfall, Yield),
    mean, na.rm = TRUE
  ))

for (col in names(crop_summary)[-1]) {  # Skip Crop
  p <- ggplot(
    crop_summary,
    aes_string(
      x = paste0("reorder(Crop, ", col, ")"),
      y = col
    )
  ) +
    geom_bar(stat = "identity", fill = "darkgreen") +
    coord_flip() +
    theme_minimal() +
    labs(
      title = paste("Average", col, "by Crop"),
      x = "Crop",
      y = col
    )
  print(p)
}
```
#Correlation matrix
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
numeric_vars <- c(
  "Nitrogen", "Phosphorus", "Potassium",
  "Temperature", "Humidity", "pH_Value",
  "Rainfall", "Yield")

corr_data <- df[, numeric_vars]
corr_matrix <- round(cor(corr_data, use = "complete.obs"), 2)

ggcorrplot(corr_matrix,
           lab = TRUE,
           title = "Correlation Matrix of Numeric Variables")
```
#Distribution plots for each variable
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Reshape data to long format for faceted plotting
df_long <- df %>% 
  select(Nitrogen, Phosphorus, Potassium,
         Temperature, Humidity, pH_Value,
         Rainfall, Yield) %>% 
  pivot_longer(
    cols = everything(),
    names_to = "Variable",
    values_to = "Value"
  )

# Faceted histogram + density plots
ggplot(df_long, aes(x = Value)) +
  geom_histogram(aes(y = ..density..), 
                 fill = "skyblue", color = "black") +
  geom_density() +
  facet_wrap(~ Variable, scales = "free", ncol = 3) +
  labs(title = "Distributions of Agronomic Variables") +
  theme_minimal()

```
The distribution plots show that several variables are noticeably skewed, which indicates the presence of extreme values and uneven spread. This motivates a closer look at skewness before modelling.


#Skewness check and transformations
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
sapply(df[sapply(df, is.numeric)], skewness, na.rm = TRUE)

df <- df %>%
  mutate(
    Yield = log(Yield + 1)
  )

# Skewness after transformation
sapply(df[sapply(df, is.numeric)], skewness, na.rm = TRUE)
```
We explored several transformations to address skewness in the data. Log-transforming the target variable Yield substantially improved model stability, while additional transformations of the predictors (e.g., log of P and K, squaring Humidity) produced no meaningful performance gains (see Appendix). We therefore retain only the log(Yield + 1) transformation, which aligns with standard practice and keeps the preprocessing pipeline simple and interpretable.


## Modeling & Benchmarking
- Define a regression task using the mlr3 framework  
- Specify four different learners:  
  - **Featureless model** (baseline)  
  - **Linear Regression**  
  - **Decision Tree (rpart)**  
  - **Random Forest (ranger)**  
- Evaluate all models using **5-fold cross-validation**  
- Compare model performance using **MSE** and **R²**  
- Visualize performance differences across models  

We first include "crop" as a predictor to establish an upper-bound benchmark, showing how well models perform when they are allowed to use the crop identity itself. This step does not answer our research question, but it highlights that most predictive power in this dataset comes from differences between crops.

#Standard CV with "Crop" as predictor
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Tase include "Crop" in the feature set
task_with_crop <- as_task_regr(
  df,                                
  id     = "crop_yield_with_crop",
  target = "Yield"
)

# Learners
fl_wc <- lrn("regr.featureless")
lm_wc <- lrn("regr.lm")
dt_wc <- lrn("regr.rpart")
rf_wc <- lrn("regr.ranger", num.trees = 500)

rdesc_wc <- rsmp("cv", folds = 5)

design_wc <- benchmark_grid(
  tasks       = task_with_crop,
  learners    = list(fl_wc, lm_wc, dt_wc, rf_wc),
  resamplings = rdesc_wc
)

bm_wc <- benchmark(design_wc)

# Performance table
bm_wc$aggregate(msrs(c("regr.rmse", "regr.mse", "regr.rsq")))

results_wc <- bm_wc$aggregate(msrs(c("regr.mse", "regr.rsq")))

# Plot MSE (with Crop)
ggplot(results_wc, aes(x = learner_id, y = regr.mse, fill = learner_id)) +
  geom_bar(stat = "identity") +
  labs(title = "MSE of Regression Algorithms (with Crop as Predictor)",
       x = "Algorithm", y = "Mean Squared Error (MSE)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")

# Plot R² (with Crop)
ggplot(results_wc, aes(x = learner_id, y = regr.rsq, fill = learner_id)) +
  geom_bar(stat = "identity") +
  labs(title = "R² of Regression Algorithms (with Crop as Predictor)",
       x = "Algorithm", y = "R-squared") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```

We then run all models without "crop", which aligns with our research goal of assessing whether soil and environmental features alone can predict yield—especially for new crop types
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Task without "Crop": matches our research focus on generalising across crops
task <- as_task_regr(df %>% dplyr::select(-Crop),
                     id = "crop_yield",
                     target = "Yield")

# learners 
fl <- lrn("regr.featureless")   #Predicts the mean of Yield in the training data for everyone
lm <- lrn("regr.lm")
dt <- lrn("regr.rpart")
rf <- lrn("regr.ranger", num.trees = 500)

rdesc <- rsmp("cv", folds = 5)

design <- benchmark_grid(
  tasks = task,
  learners = list(fl, lm, dt, rf),
  resamplings = rdesc
)

bm <- benchmark(design)

bm$aggregate(msrs(c("regr.rmse", "regr.mse", "regr.rsq")))

results <- bm$aggregate(msrs(c("regr.mse", "regr.rsq")))

# Plot MSE
ggplot(results, aes(x = learner_id, y = regr.mse, fill = learner_id)) +
  geom_bar(stat = "identity") +
  labs(title = "Comparison of Regression Algorithm MSE",
       x = "Algorithm", y = "Mean Squared Error (MSE)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")

# Plot R²
ggplot(results, aes(x = learner_id, y = regr.rsq, fill = learner_id)) +
  geom_bar(stat = "identity") +
  labs(title = "Comparison of Regression Algorithm R²",
       x = "Algorithm", y = "R-squared") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")

```
The benchmark shows that both linear regression and the random forest achieve clearly better performance than the featureless baseline, with the random forest performing best overall. The decision tree performs moderately but remains noticeably less accurate than the other two models. Compared to the earlier results where Crop was included as a predictor, all models now show slightly lower R² and higher MSE, confirming that crop identity provided useful signal. Still, the models capture meaningful patterns from the environmental and soil features alone, which aligns with our goal of assessing how well yield can be predicted without relying on Crop.

## Nested Hyperparameter Tuning for Random Forest
- Enable parallel processing to accelerate tuning  
- Define a hyperparameter search space for **mtry** and **min.node.size**  
- Run an **inner 4-fold cross-validation** loop to tune the Random Forest using random search  
- Evaluate the tuned model with an **outer 3-fold cross-validation** loop to avoid overfitting to the tuning procedure  
- Compare tuned vs. untuned Random Forest performance to assess whether tuning provides a meaningful improvement  

```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
detectCores()
cores = detectCores() - 1 # save one core to keep the computer operational
plan("multisession", workers = cores)

# Learner (same RF as before)
rf_tune <- lrn("regr.ranger", num.trees = 500)

# Hyperparameter search space
rf_ps <- ps(
  mtry          = p_int(lower = 1, upper = 5),
  min.node.size = p_int(lower = 1, upper = 50)
)

# Inner resampling + measure + tuner
res_inner  <- rsmp("cv", folds = 4)
mes_inner  <- msr("regr.mse")
terminator <- trm("evals", n_evals = 100)
tuner      <- tnr("random_search")

# AutoTuner (inner loop)
rf_at <- AutoTuner$new(
  learner     = rf_tune,
  resampling  = res_inner,
  measure     = mes_inner,
  search_space = rf_ps,
  terminator  = terminator,
  tuner       = tuner
)

# Outer resampling (nested CV)
res_outer <- rsmp("cv", folds = 3)

nested_res <- resample(
  task       = task,      # your regression task without Crop
  learner    = rf_at,
  resampling = res_outer
)

plan("sequential")  # turn off parallel

# Aggregate performance of tuned RF
# nested_res$aggregate(msrs(c("regr.rmse", "regr.mse", "regr.rsq")))

# Untuned RF from standard CV (bm)
rf_untuned_full <- bm$aggregate(msrs(c("regr.rmse", "regr.mse", "regr.rsq")))
rf_untuned <- rf_untuned_full[rf_untuned_full$learner_id == "regr.ranger",
                              c("regr.rmse", "regr.mse", "regr.rsq"),
                              drop = FALSE]
rf_untuned <- cbind(model = "RF_untuned", rf_untuned)

# Tuned RF from nested CV
rf_tuned_vec <- nested_res$aggregate(msrs(c("regr.rmse", "regr.mse", "regr.rsq")))
rf_tuned <- data.frame(model = "RF_tuned", t(rf_tuned_vec))

# Combined comparison table
rf_compare <- rbind(rf_untuned, rf_tuned, row.names = NULL)
rf_compare
```
Tuning the Random Forest with nested resampling resulted in only a marginal improvement in performance. The tuned model shows slightly lower RMSE and higher R² than the untuned version, consistent with the idea that Random Forests are already robust and require limited hyperparameter adjustment for this dataset and therefore we will continue with the untuned RF going forward. 

## Modeling & Benchmarking with Blocked Cross-Validation (by Crop)
- Create a new regression task that includes Crop as a grouping variable
- Remove Crop from the feature set so it is used only for blocking
- Define a 5-fold cross-validation strategy that respects these blocks

```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Build a new task including Crop
task_block <- as_task_regr(df,                      # df still contains Crop and transformed predictors
              id      = "crop_yield_block",
              target  = "Yield")

# "Crop" defines the groups/blocks
task_block$col_roles$group <- "Crop"

# Remove Crop from the feature set (we only use it for grouping, not as predictor)
task_block$col_roles$feature <- setdiff(task_block$col_roles$feature, "Crop")

# Check roles of variables
task_block$col_roles

# Define a 5-fold CV resampling
  # each fold's train/test split keeps whole crops together
rdesc_block <- rsmp("cv", folds = 5)

# Create a benchmark design using the same learners as before (without blocking)
design_block <- benchmark_grid(
  tasks       = task_block,
  learners    = list(fl, lm, dt, rf),
  resamplings = rdesc_block
)

# Benchmark with blocked CV
bm_block <- benchmark(design_block)

# Aggregate performance metrics (RMSE, MSE, R²) for blocked CV
bm_block$aggregate(msrs(c("regr.rmse", "regr.mse", "regr.rsq")))

results_block <- bm_block$aggregate(msrs(c("regr.mse", "regr.rsq")))

# Plot MSE for blocked CV
ggplot(results_block,
       aes(x = learner_id, y = regr.mse, fill = learner_id)) +
  geom_bar(stat = "identity") +
  labs(title = "MSE (Blocked CV by Crop)",
       x = "Algorithm",
       y = "MSE (log(Yield + 1))") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")

# Plot R² for blocked CV
ggplot(results_block,
       aes(x = learner_id, y = regr.rsq, fill = learner_id)) +
  geom_bar(stat = "identity") +
  labs(title = "R² (Blocked CV by Crop)",
       x = "Algorithm",
       y = "R-squared (log(Yield + 1))") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")

```
With blocking by Crop, all models now have higher MSE than the featureless baseline and even negative R² values, which means they perform worse than simply predicting the mean yield for every observation. This shows that the patterns learned from soil and environmental variables do not transfer well to entirely new crop types, in sharp contrast to the more optimistic unblocked cross validation. In terms of our research question, this suggests that while yield can be modelled reasonably within known crops, predictive accuracy drops sharply once we require generalisation to crops the model has never seen before.


## Variable Importance

#Models without blocking 
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Train learners once on the entire data set 
lm_model  <- lm$train(task)   # linear regression model
tree_model <- dt$train(task)  # regression tree model
rf_model   <- rf$train(task)  # random forest model (used later, but we train here once)

## Linear Regression:
# Extract regression coefficients of the linear regression model 
beta_weights <- coef(lm_model$model)  

# get beta coefficients
beta_df <- data.frame(
  Feature     = names(beta_weights),
  Coefficient = as.numeric(beta_weights)
)

# Plot the coefficients 
betas_plot <- ggplot(beta_df, aes(x = reorder(Feature, Coefficient),
                                  y = Coefficient)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Linear Regression Coefficients (log(Yield + 1))",
       x = "Feature",
       y = "Coefficient (Beta)") +
  theme_minimal()

plot(betas_plot)

## Decision Tree:
# Plot the decicion tree 
rpart.plot(tree_model$model,
           main = "Regression Tree for log(Yield + 1)")

## Random Forest: 
# Compute permutation variable importance for rf model 
# define the target variables and the predictors 
y <- df$Yield   # log-transformed yield: log(Yield + 1)
X <- df[, setdiff(names(df), c("Yield", "Crop"))]  # all predictors used in 'task'

mod <- Predictor$new(rf_model, data = X, y = y) # Create Predictor object

importance <- FeatureImp$new(mod, loss = "mse", n.repetitions = 10) # run permutations
importance$plot() + 
  ggtitle("Feature Importance") # Plot the importance
```
The linear regression coefficients are very small, indicating that simple linear effects of the predictors on yield are weak. The regression tree, in contrast, mainly splits on humidity, potassium, rainfall and phosphorus, revealing clear threshold based, nonlinear effects. The random forest importance plot broadly agrees with the tree, with humidity and rainfall most important and nitrogen least important, suggesting that these few environmental variables drive most of the predictive structure in the unblocked setting.

#Models with blocking (by Crop)
```{r, echo=TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# For blocked models, we use a separate set of learners to avoid overwriting
lm_block  <- lrn("regr.lm")
dt_block  <- lrn("regr.rpart")
rf_block  <- lrn("regr.ranger", num.trees = 500)

# Train on the blocked task (Crop as group, but not a feature)
lm_model_block   <- lm_block$train(task_block)
tree_model_block <- dt_block$train(task_block)
rf_model_block   <- rf_block$train(task_block)

## Linear Regression (blocked)
# Extract regression coefficients of the linear regression model 
beta_weights_block <- coef(lm_model_block$model) 

# get beta coefficients
beta_block_df <- data.frame(
  Feature     = names(beta_weights_block),
  Coefficient = as.numeric(beta_weights_block)
)

# Plot the coefficients 
betas_plot_block <- ggplot(beta_block_df, aes(x = reorder(Feature, Coefficient),
                                              y = Coefficient)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Linear Regression Coefficients (log(Yield + 1), blocked by Crop)",
       x = "Feature",
       y = "Coefficient (Beta)") +
  theme_minimal()

plot(betas_plot_block)

## Decision Tree (blocked)
# Plot the decicion tree 
rpart.plot(tree_model_block$model,
           main = "Regression Tree for log(Yield + 1) (blocked by Crop)")

## Random Forest (blocked, permutation importance via iml)
# Predictors and target are the same columns; blocking only affects resampling, not X/y here
y_block <- df$Yield
X_block <- df[, setdiff(names(df), c("Yield", "Crop"))]

mod_block <- Predictor$new(rf_model_block, data = X_block, y = y_block) # Create Predictor object
importance_block <- FeatureImp$new(mod_block, loss = "mse", n.repetitions = 10) # run permutations
importance_block$plot() + 
  ggtitle("Feature Importance (blocked by Crop)") # Plot the importance

```
With blocking by Crop, the three models again point to very similar drivers as in the unblocked setting: the linear regression coefficients remain tiny, the regression tree still splits mainly on humidity, potassium, rainfall and phosphorus, and the random forest ranks humidity and rainfall highest while nitrogen is least important. This stability suggests that the same soil and environmental variables are informative both within and across crops. However, taken together with the strongly reduced predictive performance under blocking, these importance patterns do not translate into accurate yield predictions for entirely new crop types.



# Implications, Limitations, and Outlook
Our analysis shows that crop yield can be predicted reasonably well from environmental and soil characteristics when the models are evaluated within known crop types. In the standard cross-validation setting, both linear and nonlinear models outperform the featureless baseline, and the random forest captures the strongest structure in the data. The variable-importance results highlight humidity, rainfall, pH, temperature, potassium and phosphorus as the factors that consistently contribute the most predictive signal, suggesting that these environmental conditions systematically influence yield across crops. For practical agriculture, this implies that machine-learning models trained on historical soil and climate variables may support short-term planning tasks such as irrigation, fertilization, or local risk assessment, as long as the crop types remain similar to those seen in the training data.

However, the blocked cross-validation results reveal an important limitation. When entire crop types are held out during training, model accuracy drops sharply, with higher MSE and negative R² values across all learners. This indicates that the relationships learned from environmental variables do not generalize reliably to crop types the model has never encountered. Even though the same variables appear important in the blocked setting, they no longer translate into meaningful predictive performance. This suggests that yield–environment relationships may be crop-specific, and that simple tabular features from this dataset are insufficient for robust cross-crop generalization. As an upper-bound benchmark, we also showed that including Crop itself as a predictor dramatically improves performance, which confirms that crop identity carries very strong predictive information and aligns with the drastic performance drop seen in the blocked setting. Although this upper-bound benchmark does not answer our research question, it reinforces the conclusion that generalizing across crops is fundamentally harder than predicting within crops.

These findings point to several directions for future work. One avenue is to enrich the feature space with mechanistic or physiological variables, satellite-based vegetation indices, or temporal weather summaries that may capture crop-agnostic patterns more effectively. Another is to explore hierarchical or mixed-effect models that explicitly represent crop-level variation rather than trying to learn a single global mapping. Larger and more diverse datasets spanning more crop types and environmental gradients may also help the models learn more transferable relationships. Finally, even though cross-crop generalization remains challenging, the consistent importance of humidity, rainfall, and related features suggests that models can still provide agronomically meaningful insights and support targeted decision-making within the scope of crop types they were trained on.
